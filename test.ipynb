{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all the submissions from the day \n",
    "#get 10 with most twitter citations\n",
    "#summarize and post them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yesterday's date in ET: 2024-08-30\n",
      "https://www.biorxiv.org/content/early/recent?page=0\n",
      "page date:  2024-08-30\n",
      "2024-08-30\n",
      "https://www.biorxiv.org/content/early/recent?page=1\n",
      "page date:  2024-08-30\n",
      "2024-08-30\n",
      "https://www.biorxiv.org/content/early/recent?page=2\n",
      "page date:  2024-08-30\n",
      "2024-08-30\n",
      "https://www.biorxiv.org/content/early/recent?page=3\n",
      "page date:  2024-08-30\n",
      "2024-08-30\n",
      "https://www.biorxiv.org/content/early/recent?page=4\n",
      "page date:  2024-08-30\n",
      "2024-08-30\n",
      "https://www.biorxiv.org/content/early/recent?page=5\n",
      "page date:  2024-08-30\n",
      "2024-08-30\n",
      "https://www.biorxiv.org/content/early/recent?page=6\n",
      "page date:  2024-08-30\n",
      "2024-08-30\n",
      "https://www.biorxiv.org/content/early/recent?page=7\n",
      "page date:  2024-08-30\n",
      "2024-08-30\n",
      "https://www.biorxiv.org/content/early/recent?page=8\n",
      "page date:  2024-08-30\n",
      "2024-08-30\n",
      "https://www.biorxiv.org/content/early/recent?page=9\n",
      "page date:  2024-08-30\n",
      "2024-08-30\n",
      "https://www.biorxiv.org/content/early/recent?page=10\n",
      "page date:  2024-08-30\n",
      "2024-08-30\n",
      "https://www.biorxiv.org/content/early/recent?page=11\n",
      "page date:  2024-08-30\n",
      "2024-08-30\n",
      "https://www.biorxiv.org/content/early/recent?page=12\n",
      "page date:  2024-08-30\n",
      "2024-08-30\n",
      "https://www.biorxiv.org/content/early/recent?page=13\n",
      "page date:  2024-08-30\n",
      "2024-08-30\n",
      "https://www.biorxiv.org/content/early/recent?page=14\n",
      "page date:  2024-08-30\n",
      "2024-08-30\n",
      "https://www.biorxiv.org/content/early/recent?page=15\n",
      "page date:  2024-08-30\n",
      "2024-08-30\n",
      "https://www.biorxiv.org/content/early/recent?page=16\n",
      "page date:  2024-08-30\n",
      "2024-08-30\n",
      "https://www.biorxiv.org/content/early/recent?page=17\n",
      "page date:  2024-08-30\n",
      "2024-08-30\n",
      "https://www.biorxiv.org/content/early/recent?page=18\n",
      "page date:  2024-08-30\n",
      "2024-08-30\n",
      "https://www.biorxiv.org/content/early/recent?page=19\n",
      "page date:  2024-08-30\n",
      "2024-08-30\n",
      "https://www.biorxiv.org/content/early/recent?page=20\n",
      "page date:  2024-08-30\n",
      "2024-08-30\n",
      "https://www.biorxiv.org/content/early/recent?page=21\n",
      "page date:  2024-08-30\n",
      "2024-08-30\n",
      "https://www.biorxiv.org/content/early/recent?page=22\n",
      "page date:  2024-08-30\n",
      "2024-08-30\n",
      "https://www.biorxiv.org/content/early/recent?page=23\n",
      "page date:  2024-08-30\n",
      "2024-08-30\n",
      "https://www.biorxiv.org/content/early/recent?page=24\n",
      "page date:  2024-08-30\n",
      "2024-08-30\n",
      "https://www.biorxiv.org/content/early/recent?page=25\n",
      "page date:  2024-08-30\n",
      "2024-08-30\n",
      "https://www.biorxiv.org/content/early/recent?page=26\n",
      "page date:  2024-08-29\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "based on ET - every day one hour past 00:00 past ET i will tweet it \n",
    "\n",
    "go to certain website \n",
    "\n",
    "1. get date of day before \n",
    "2. open first page and check if date is equals to target date if yes add all the paper urls from the page to the list and if date is older then target date exit\n",
    "3. open all pages, get the number of tweet impressions, get the url to the pdf, title and subject area\n",
    "4. make summary and post\n",
    "\"\"\"\n",
    "from datetime import datetime, timedelta\n",
    "import pytz\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "# Get date from yesterday ET time\n",
    "eastern = pytz.timezone('US/Eastern')\n",
    "yesterday = datetime.now(eastern) - timedelta(days=1)\n",
    "yesterday_date = yesterday.strftime('%Y-%m-%d')\n",
    "print(\"Yesterday's date in ET:\", yesterday_date)\n",
    "\n",
    "page_data = []\n",
    "\n",
    "def iterate_biorxiv_pages():\n",
    "    page_number = 0\n",
    "    while True:\n",
    "        url = f\"https://www.biorxiv.org/content/early/recent?page={page_number}\"\n",
    "        print(url)\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Get the date based on the X path\n",
    "        date_element = soup.select_one('#block-system-main > div > div > div > div:nth-child(2) > div:nth-child(1) > div > div > div > div > div:nth-child(1) > h3')\n",
    "        date_text = date_element.get_text(strip=True)\n",
    "        \n",
    "        # Reformat the date to match the format of our yesterday_date\n",
    "        page_date = datetime.strptime(date_text, '%B %d, %Y').strftime('%Y-%m-%d')\n",
    "\n",
    "        print(\"page date: \", page_date)\n",
    "        \n",
    "        # Check if the date is older than the target date\n",
    "        if page_date < yesterday_date:\n",
    "            break\n",
    "        \n",
    "        # Skip the page if the date is newer than the target date\n",
    "        if page_date > yesterday_date:\n",
    "            page_number += 1\n",
    "            continue\n",
    "\n",
    "        # Extract all the URLs from the page\n",
    "        article_blocks = soup.find_all('div', class_='highwire-cite highwire-cite-highwire-article highwire-citation-biorxiv-article-pap-list-overline clearfix')\n",
    "        \n",
    "        for block in article_blocks:\n",
    "            title_element = block.find('span', class_='highwire-cite-title')\n",
    "            if title_element:\n",
    "                link_element = title_element.find('a', class_='highwire-cite-linked-title')\n",
    "                if link_element and 'href' in link_element.attrs:\n",
    "                    paper_url = \"https://www.biorxiv.org\" + link_element['href']\n",
    "                    page_data.append(paper_url)\n",
    "        \n",
    "        #here i can extract all the data\n",
    "\n",
    "        # If date is target date, get all the data and save it in list\n",
    "        print(page_date)\n",
    "        # Add logic to extract and save data here\n",
    "        \n",
    "        page_number += 1\n",
    "    return page_data\n",
    "\n",
    "page_data = iterate_biorxiv_pages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch: ['https://www.biorxiv.org/content/10.1101/2024.08.30.610472v1', 'https://www.biorxiv.org/content/10.1101/2024.08.30.610471v1']\n",
      "Fetching URL: https://www.biorxiv.org/content/10.1101/2024.08.30.610472v1\n",
      "Fetching URL: https://www.biorxiv.org/content/10.1101/2024.08.30.610471v1\n",
      "Parsed page date: 2024-08-30\n",
      "Date matches target date: 2024-08-30\n",
      "tweet_element:  <span id=\"count_twitter\">2</span>\n",
      "Extracted tweet count: 2\n",
      "Parsed page date: 2024-08-30\n",
      "Date matches target date: 2024-08-30\n",
      "tweet_element:  <span id=\"count_twitter\">1</span>\n",
      "Extracted tweet count: 1\n",
      "Processing batch: ['https://www.biorxiv.org/content/10.1101/2024.08.30.609158v1', 'https://www.biorxiv.org/content/10.1101/2024.08.30.609923v1']\n",
      "Fetching URL: https://www.biorxiv.org/content/10.1101/2024.08.30.609158v1\n",
      "Fetching URL: https://www.biorxiv.org/content/10.1101/2024.08.30.609923v1\n",
      "Parsed page date: 2024-08-30\n",
      "Date matches target date: 2024-08-30\n",
      "tweet_element:  <span id=\"count_twitter\">2</span>\n",
      "Extracted tweet count: 2\n",
      "Parsed page date: 2024-08-30\n",
      "Date matches target date: 2024-08-30\n",
      "tweet_element:  <span id=\"count_twitter\">3</span>\n",
      "Extracted tweet count: 3\n",
      "Processing batch: ['https://www.biorxiv.org/content/10.1101/2024.08.30.610437v1', 'https://www.biorxiv.org/content/10.1101/2024.08.30.610386v1']\n",
      "Fetching URL: https://www.biorxiv.org/content/10.1101/2024.08.30.610437v1\n",
      "Fetching URL: https://www.biorxiv.org/content/10.1101/2024.08.30.610386v1\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from playwright.async_api import async_playwright, TimeoutError as PlaywrightTimeoutError\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timedelta\n",
    "import nest_asyncio\n",
    "\n",
    "# Apply the nest_asyncio patch\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Initialize an empty list to store the dictionaries\n",
    "tweet_data_list = []\n",
    "\n",
    "# Function to generate the target date\n",
    "def get_yesterday_date():\n",
    "    yesterday = datetime.now() - timedelta(days=1)\n",
    "    return yesterday.strftime('%Y-%m-%d')\n",
    "\n",
    "yesterday_date = get_yesterday_date()\n",
    "\n",
    "# Asynchronous function to fetch and parse a single URL\n",
    "async def fetch_and_parse(url, context):\n",
    "    print(f\"Fetching URL: {url}\")\n",
    "    retries = 3\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            page = await context.new_page()\n",
    "            await page.goto(url, timeout=60000)  # Wait up to 60 seconds for the page to load\n",
    "            await page.wait_for_selector('#count_twitter', timeout=60000)  # Wait up to 60 seconds for the element\n",
    "            content = await page.content()\n",
    "            await page.close()\n",
    "            break  # Exit the retry loop if successful\n",
    "        except PlaywrightTimeoutError as e:\n",
    "            print(f\"Timeout error on {url}: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error on {url}: {e}\")\n",
    "        if attempt < retries - 1:\n",
    "            print(f\"Retrying {url} (attempt {attempt + 1}/{retries})\")\n",
    "        else:\n",
    "            print(f\"Failed to fetch {url} after {retries} attempts\")\n",
    "            return\n",
    "\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "    \n",
    "    # Extract the date from the page\n",
    "    date_element = soup.select_one('#block-system-main > div > div > div > div > div:nth-child(2) > div > div > div:nth-child(3) > div')\n",
    "    if date_element:\n",
    "        date_text = date_element.get_text(strip=True)\n",
    "        # Remove the 'Posted\\xa0' prefix if it exists\n",
    "        if date_text.startswith('Posted\\xa0'):\n",
    "            date_text = date_text.replace('Posted\\xa0', '')\n",
    "        # Remove the period at the end of the date string\n",
    "        if date_text.endswith('.'):\n",
    "            date_text = date_text[:-1]\n",
    "        try:\n",
    "            page_date = datetime.strptime(date_text, '%B %d, %Y').strftime('%Y-%m-%d')\n",
    "            print(f\"Parsed page date: {page_date}\")  # Debug print\n",
    "        except ValueError as e:\n",
    "            print(f\"Error parsing date on {url}: {e}\")\n",
    "            return\n",
    "        \n",
    "        # Check if the date matches the target date\n",
    "        if page_date == yesterday_date:\n",
    "            print(f\"Date matches target date: {page_date}\")  # Debug print\n",
    "            # Extract the number of tweets using a CSS selector\n",
    "            tweet_element = soup.select_one('#count_twitter')\n",
    "            print(\"tweet_element: \", tweet_element)  # Debug print\n",
    "            if tweet_element:\n",
    "                tweet_count = tweet_element.get_text(strip=True)\n",
    "                print(f\"Extracted tweet count: {tweet_count}\")  # Debug print\n",
    "                \n",
    "                # Create a dictionary with the URL as key and tweet count as value\n",
    "                tweet_data = {url: tweet_count}\n",
    "                \n",
    "                # Add the dictionary to the list\n",
    "                tweet_data_list.append(tweet_data)\n",
    "            else:\n",
    "                print(f\"Tweet element not found on {url}\")\n",
    "        else:\n",
    "            print(f\"Date does not match target date on {url}\")\n",
    "    else:\n",
    "        print(f\"Date element not found on {url}\")\n",
    "\n",
    "# Main asynchronous function to handle multiple requests in batches\n",
    "async def main(urls, batch_size=2, delay=5):\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "        \n",
    "        for i in range(0, len(urls), batch_size):\n",
    "            batch = urls[i:i + batch_size]\n",
    "            print(f\"Processing batch: {batch}\")\n",
    "            context = await browser.new_context()\n",
    "            tasks = [fetch_and_parse(url, context) for url in batch]\n",
    "            await asyncio.gather(*tasks)\n",
    "            await context.close()\n",
    "            await asyncio.sleep(delay)  # Add a delay between batches\n",
    "        \n",
    "        await browser.close()\n",
    "\n",
    "# List of URLs to process\n",
    "urls = page_data\n",
    "\n",
    "# Run the main function\n",
    "await main(urls)\n",
    "\n",
    "# Print the list of dictionaries\n",
    "print(tweet_data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Request the page\n",
    "response = requests.get('https://www.biorxiv.org/content/10.1101/2024.08.28.610160v1')\n",
    "\n",
    "# Load the page into a BeautifulSoup object\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "with open('page_content.html', 'w', encoding='utf-8') as file:\n",
    "    file.write(soup.prettify())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from playwright.async_api import async_playwright\n",
    "\n",
    "async def save_page_content(url, file_path):\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "        page = await browser.new_page()\n",
    "        await page.goto(url)\n",
    "        content = await page.content()\n",
    "        await browser.close()\n",
    "        \n",
    "        with open(file_path, 'w', encoding='utf-8') as file:\n",
    "            file.write(content)\n",
    "\n",
    "# URL to load\n",
    "url = 'https://www.biorxiv.org/content/10.1101/2024.08.28.610160v1'\n",
    "# File path to save the content\n",
    "file_path = 'page_content.html'\n",
    "\n",
    "# Run the async function\n",
    "asyncio.run(save_page_content(url, file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
