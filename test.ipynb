{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all the submissions from the day \n",
    "#get 10 with most twitter citations\n",
    "#summarize and post them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yesterday's date in ET: 2024-08-30\n",
      "https://www.biorxiv.org/content/early/recent?page=0\n",
      "page date:  2024-08-30\n",
      "2024-08-30\n",
      "https://www.biorxiv.org/content/early/recent?page=1\n",
      "page date:  2024-08-30\n",
      "2024-08-30\n",
      "https://www.biorxiv.org/content/early/recent?page=2\n",
      "page date:  2024-08-30\n",
      "2024-08-30\n",
      "https://www.biorxiv.org/content/early/recent?page=3\n",
      "page date:  2024-08-30\n",
      "2024-08-30\n",
      "https://www.biorxiv.org/content/early/recent?page=4\n",
      "page date:  2024-08-30\n",
      "2024-08-30\n",
      "https://www.biorxiv.org/content/early/recent?page=5\n",
      "page date:  2024-08-30\n",
      "2024-08-30\n",
      "https://www.biorxiv.org/content/early/recent?page=6\n",
      "page date:  2024-08-30\n",
      "2024-08-30\n",
      "https://www.biorxiv.org/content/early/recent?page=7\n",
      "page date:  2024-08-30\n",
      "2024-08-30\n",
      "https://www.biorxiv.org/content/early/recent?page=8\n",
      "page date:  2024-08-30\n",
      "2024-08-30\n",
      "https://www.biorxiv.org/content/early/recent?page=9\n",
      "page date:  2024-08-30\n",
      "2024-08-30\n",
      "https://www.biorxiv.org/content/early/recent?page=10\n",
      "page date:  2024-08-30\n",
      "2024-08-30\n",
      "https://www.biorxiv.org/content/early/recent?page=11\n",
      "page date:  2024-08-30\n",
      "2024-08-30\n",
      "https://www.biorxiv.org/content/early/recent?page=12\n",
      "page date:  2024-08-30\n",
      "2024-08-30\n",
      "https://www.biorxiv.org/content/early/recent?page=13\n",
      "page date:  2024-08-30\n",
      "2024-08-30\n",
      "https://www.biorxiv.org/content/early/recent?page=14\n",
      "page date:  2024-08-30\n",
      "2024-08-30\n",
      "https://www.biorxiv.org/content/early/recent?page=15\n",
      "page date:  2024-08-30\n",
      "2024-08-30\n",
      "https://www.biorxiv.org/content/early/recent?page=16\n",
      "page date:  2024-08-30\n",
      "2024-08-30\n",
      "https://www.biorxiv.org/content/early/recent?page=17\n",
      "page date:  2024-08-30\n",
      "2024-08-30\n",
      "https://www.biorxiv.org/content/early/recent?page=18\n",
      "page date:  2024-08-30\n",
      "2024-08-30\n",
      "https://www.biorxiv.org/content/early/recent?page=19\n",
      "page date:  2024-08-30\n",
      "2024-08-30\n",
      "https://www.biorxiv.org/content/early/recent?page=20\n",
      "page date:  2024-08-30\n",
      "2024-08-30\n",
      "https://www.biorxiv.org/content/early/recent?page=21\n",
      "page date:  2024-08-30\n",
      "2024-08-30\n",
      "https://www.biorxiv.org/content/early/recent?page=22\n",
      "page date:  2024-08-30\n",
      "2024-08-30\n",
      "https://www.biorxiv.org/content/early/recent?page=23\n",
      "page date:  2024-08-30\n",
      "2024-08-30\n",
      "https://www.biorxiv.org/content/early/recent?page=24\n",
      "page date:  2024-08-30\n",
      "2024-08-30\n",
      "https://www.biorxiv.org/content/early/recent?page=25\n",
      "page date:  2024-08-30\n",
      "2024-08-30\n",
      "https://www.biorxiv.org/content/early/recent?page=26\n",
      "page date:  2024-08-29\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "based on ET - every day one hour past 00:00 past ET i will tweet it \n",
    "\n",
    "go to certain website \n",
    "\n",
    "1. get date of day before \n",
    "2. open first page and check if date is equals to target date if yes add all the paper urls from the page to the list and if date is older then target date exit\n",
    "3. open all pages, get the number of tweet impressions, get the url to the pdf, title and subject area\n",
    "4. make summary and post\n",
    "\"\"\"\n",
    "from datetime import datetime, timedelta\n",
    "import pytz\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "# Get date from yesterday ET time\n",
    "eastern = pytz.timezone('US/Eastern')\n",
    "yesterday = datetime.now(eastern) - timedelta(days=1)\n",
    "yesterday_date = yesterday.strftime('%Y-%m-%d')\n",
    "print(\"Yesterday's date in ET:\", yesterday_date)\n",
    "\n",
    "\n",
    "def iterate_biorxiv_pages():\n",
    "    page_number = 0\n",
    "    while True:\n",
    "        url = f\"https://www.biorxiv.org/content/early/recent?page={page_number}\"\n",
    "        print(url)\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Get the date based on the X path\n",
    "        date_element = soup.select_one('#block-system-main > div > div > div > div:nth-child(2) > div:nth-child(1) > div > div > div > div > div:nth-child(1) > h3')\n",
    "        date_text = date_element.get_text(strip=True)\n",
    "        \n",
    "        # Reformat the date to match the format of our yesterday_date\n",
    "        page_date = datetime.strptime(date_text, '%B %d, %Y').strftime('%Y-%m-%d')\n",
    "\n",
    "        print(\"page date: \", page_date)\n",
    "        \n",
    "        # Check if the date is older than the target date\n",
    "        if page_date < yesterday_date:\n",
    "            break\n",
    "        \n",
    "        # Skip the page if the date is newer than the target date\n",
    "        if page_date > yesterday_date:\n",
    "            page_number += 1\n",
    "            continue\n",
    "\n",
    "        # If date is target date, get all the data and save it in list\n",
    "        print(page_date)\n",
    "        # Add logic to extract and save data here\n",
    "        \n",
    "        page_number += 1\n",
    "        time.sleep(1)\n",
    "        #now i can get the date\n",
    "\n",
    "        # if response.status_code != 200:\n",
    "        #     print(f\"Failed to retrieve page {page_number}. Status code: {response.status_code}\")\n",
    "        #     break\n",
    "        \n",
    "        # soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        # articles = soup.find_all('div', class_='highwire-article-citation')\n",
    "        \n",
    "        # if not articles:\n",
    "        #     print(f\"No articles found on page {page_number}. Ending pagination.\")\n",
    "        #     break\n",
    "        \n",
    "        # print(f\"Page {page_number} contains {len(articles)} articles.\")\n",
    "        \n",
    "\n",
    "\n",
    "iterate_biorxiv_pages()\n",
    "\n",
    "\n",
    "\n",
    "#next once ive got all the data i can process it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from playwright.async_api import async_playwright, TimeoutError\n",
    "import nest_asyncio\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import date, timedelta\n",
    "\n",
    "# Function to generate the Hugging Face URL\n",
    "def generate_huggingface_url():\n",
    "    yesterday = date.today() - timedelta(days=1)\n",
    "    url = f\"https://huggingface.co/papers?date={yesterday.year}-{yesterday.month:02d}-{yesterday.day:02d}\"\n",
    "    return url\n",
    "\n",
    "# Async function to fetch the HTML content\n",
    "async def fetch_papers():\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "        page = await browser.new_page()\n",
    "        huggingface_url = generate_huggingface_url()\n",
    "        await page.goto(huggingface_url)\n",
    "        html_content = await page.content()\n",
    "        papers = parse_html_content(html_content)\n",
    "        return papers\n",
    "\n",
    "# Function to parse the HTML content and extract paper details\n",
    "def parse_html_content(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    papers = []\n",
    "    for container in soup.find_all('div', class_='from-gray-50-to-white -mt-2 flex bg-gradient-to-b px-6 pb-6 pt-8'):\n",
    "        link = container.find('a', class_='line-clamp-3 cursor-pointer text-balance')\n",
    "        if link:\n",
    "            href = link.get('href')\n",
    "            arxiv_url = f\"https://arxiv.org/pdf{href[href.rfind('/'):]}.pdf\"\n",
    "            paper_name = link.text.strip()\n",
    "            upvotes_div = container.find('div', class_='shadow-alternate').find('div', class_='leading-none')\n",
    "            if upvotes_div:\n",
    "                upvotes = upvotes_div.text.strip()\n",
    "                papers.append((arxiv_url, paper_name, upvotes))\n",
    "        else:\n",
    "            print(\"Link not found in this container.\")\n",
    "    return papers\n",
    "\n",
    "# Ensure nest_asyncio is applied\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
