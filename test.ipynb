{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import pytz\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import asyncio\n",
    "from playwright.async_api import (\n",
    "    async_playwright,\n",
    "    TimeoutError as PlaywrightTimeoutError,\n",
    ")\n",
    "import nest_asyncio\n",
    "\n",
    "# Apply the nest_asyncio patch\n",
    "nest_asyncio.apply()\n",
    "\n",
    "\n",
    "# Function to get date from yesterday ET time\n",
    "def get_yesterday_date():\n",
    "    eastern = pytz.timezone(\"US/Eastern\")\n",
    "    yesterday = datetime.now(eastern) - timedelta(days=1)\n",
    "    return yesterday.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "\n",
    "yesterday_date = get_yesterday_date()\n",
    "print(\"Yesterday's date in ET:\", yesterday_date)\n",
    "\n",
    "\n",
    "# Function to iterate through biorxiv pages and collect paper URLs\n",
    "def iterate_biorxiv_pages():\n",
    "    page_data = []\n",
    "    page_number = 0\n",
    "    while True:\n",
    "        url = f\"https://www.biorxiv.org/content/early/recent?page={page_number}\"\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        # Get the date based on the X path\n",
    "        date_element = soup.select_one(\n",
    "            \"#block-system-main > div > div > div > div:nth-child(2) > div:nth-child(1) > div > div > div > div > div:nth-child(1) > h3\"\n",
    "        )\n",
    "        print(\"date_element\", date_element)\n",
    "        if not date_element:\n",
    "            break\n",
    "        date_text = date_element.get_text(strip=True)\n",
    "\n",
    "        # Reformat the date to match the format of our yesterday_date\n",
    "        page_date = datetime.strptime(date_text, \"%B %d, %Y\").strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        print(\"page date: \", page_date)\n",
    "\n",
    "        # Check if the date is older than the target date\n",
    "        if page_date < yesterday_date:\n",
    "            break\n",
    "\n",
    "        # Skip the page if the date is newer than the target date\n",
    "        if page_date > yesterday_date:\n",
    "            page_number += 1\n",
    "            continue\n",
    "\n",
    "        # Extract all the URLs from the page\n",
    "        article_blocks = soup.find_all(\n",
    "            \"div\",\n",
    "            class_=\"highwire-cite highwire-cite-highwire-article highwire-citation-biorxiv-article-pap-list-overline clearfix\",\n",
    "        )\n",
    "\n",
    "        for block in article_blocks:\n",
    "            title_element = block.find(\"span\", class_=\"highwire-cite-title\")\n",
    "            if title_element:\n",
    "                link_element = title_element.find(\n",
    "                    \"a\", class_=\"highwire-cite-linked-title\"\n",
    "                )\n",
    "                if link_element and \"href\" in link_element.attrs:\n",
    "                    paper_url = \"https://www.biorxiv.org\" + link_element[\"href\"]\n",
    "                    page_data.append(paper_url)\n",
    "\n",
    "        page_number += 1\n",
    "    return page_data\n",
    "\n",
    "\n",
    "# Initialize an empty list to store the dictionaries\n",
    "tweet_data_list = []\n",
    "\n",
    "\n",
    "# Asynchronous function to fetch and parse a single URL\n",
    "async def fetch_and_parse(url, context):\n",
    "    print(f\"Fetching URL: {url}\")\n",
    "    retries = 3\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            page = await context.new_page()\n",
    "            await page.goto(\n",
    "                url, timeout=60000\n",
    "            )  # Wait up to 60 seconds for the page to load\n",
    "            await page.wait_for_selector(\n",
    "                \"#count_twitter\", timeout=60000\n",
    "            )  # Wait up to 60 seconds for the element\n",
    "            content = await page.content()\n",
    "            await page.close()\n",
    "            break  # Exit the retry loop if successful\n",
    "        except PlaywrightTimeoutError as e:\n",
    "            print(f\"Timeout error on {url}: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error on {url}: {e}\")\n",
    "        if attempt < retries - 1:\n",
    "            print(f\"Retrying {url} (attempt {attempt + 1}/{retries})\")\n",
    "        else:\n",
    "            print(f\"Failed to fetch {url} after {retries} attempts\")\n",
    "            return\n",
    "\n",
    "    soup = BeautifulSoup(content, \"html.parser\")\n",
    "\n",
    "    # Extract the date from the page\n",
    "    date_element = soup.select_one(\n",
    "        \"#block-system-main > div > div > div > div > div:nth-child(2) > div > div > div:nth-child(3) > div\"\n",
    "    )\n",
    "    if date_element:\n",
    "        date_text = date_element.get_text(strip=True)\n",
    "        # Remove the 'Posted\\xa0' prefix if it exists\n",
    "        if date_text.startswith(\"Posted\\xa0\"):\n",
    "            date_text = date_text.replace(\"Posted\\xa0\", \"\")\n",
    "        # Remove the period at the end of the date string\n",
    "        if date_text.endswith(\".\"):\n",
    "            date_text = date_text[:-1]\n",
    "        try:\n",
    "            page_date = datetime.strptime(date_text, \"%B %d, %Y\").strftime(\"%Y-%m-%d\")\n",
    "            print(f\"Parsed page date: {page_date}\")  # Debug print\n",
    "        except ValueError as e:\n",
    "            print(f\"Error parsing date on {url}: {e}\")\n",
    "            return\n",
    "\n",
    "        # Check if the date matches the target date\n",
    "        if page_date == yesterday_date:\n",
    "            print(f\"Date matches target date: {page_date}\")  # Debug print\n",
    "\n",
    "            # Extract the number of tweets using a CSS selector\n",
    "            tweet_element = soup.select_one(\"#count_twitter\")\n",
    "            tweet_count = tweet_element.get_text(strip=True) if tweet_element else \"0\"\n",
    "            print(f\"Extracted tweet count: {tweet_count}\")  # Debug print\n",
    "\n",
    "            # Extract the abstract\n",
    "            abstract_element = soup.select_one(\"#p-3\")\n",
    "            abstract = (\n",
    "                abstract_element.get_text(strip=True) if abstract_element else \"N/A\"\n",
    "            )\n",
    "            print(f\"Extracted abstract: {abstract}\")  # Debug print\n",
    "\n",
    "            # Extract the title\n",
    "            title_element = soup.select_one(\"#page-title\")\n",
    "            title = title_element.get_text(strip=True) if title_element else \"N/A\"\n",
    "            print(f\"Extracted title: {title}\")  # Debug print\n",
    "\n",
    "            # Extract the subject area\n",
    "            subject_area_elements = soup.select(\n",
    "                \"#block-system-main > div > div > div > div > div:nth-child(2) > div > div > div:nth-child(11) > div > div > div > ul > li > span > a\"\n",
    "            )\n",
    "            subject_area = (\n",
    "                \", \".join(\n",
    "                    [element.get_text(strip=True) for element in subject_area_elements]\n",
    "                )\n",
    "                if subject_area_elements\n",
    "                else \"N/A\"\n",
    "            )\n",
    "            print(f\"Extracted subject area: {subject_area}\")  # Debug print\n",
    "\n",
    "            # Create a dictionary with the extracted data\n",
    "            tweet_data = {\n",
    "                \"url\": url,\n",
    "                \"tweet_count\": tweet_count,\n",
    "                \"abstract\": abstract,\n",
    "                \"title\": title,\n",
    "                \"subject_area\": subject_area,\n",
    "            }\n",
    "\n",
    "            # Add the dictionary to the list\n",
    "            tweet_data_list.append(tweet_data)\n",
    "        else:\n",
    "            print(f\"Date does not match target date on {url}\")\n",
    "    else:\n",
    "        print(f\"Date element not found on {url}\")\n",
    "\n",
    "\n",
    "# Main asynchronous function to handle multiple requests in batches\n",
    "async def main(urls, batch_size=50, delay=5):\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "\n",
    "        for i in range(0, len(urls), batch_size):\n",
    "            batch = urls[i : i + batch_size]\n",
    "            print(f\"Processing batch: {batch}\")\n",
    "            context = await browser.new_context()\n",
    "            tasks = [fetch_and_parse(url, context) for url in batch]\n",
    "            await asyncio.gather(*tasks)\n",
    "            await context.close()\n",
    "            await asyncio.sleep(delay)  # Add a delay between batches\n",
    "\n",
    "        await browser.close()\n",
    "\n",
    "\n",
    "# Function to find the top ten URLs with the most tweets\n",
    "def get_top_ten_tweets(tweet_data_list):\n",
    "    # Sort the list in descending order based on tweet count\n",
    "    sorted_tweets = sorted(\n",
    "        tweet_data_list, key=lambda x: int(x[\"tweet_count\"]), reverse=True\n",
    "    )\n",
    "    # Select the top ten URLs with the most tweets\n",
    "    top_ten_tweets = sorted_tweets[:10]\n",
    "    return top_ten_tweets\n",
    "\n",
    "\n",
    "# Main function to run the entire process\n",
    "def get_trending_urls():\n",
    "    \n",
    "    urls = iterate_biorxiv_pages()\n",
    "    asyncio.run(main(urls))\n",
    "    print(\"urls\", urls)\n",
    "    return get_top_ten_tweets(tweet_data_list)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    top_ten_tweets = get_trending_urls()\n",
    "    print(\"Top ten URLs with the most tweets:\")\n",
    "    for data in top_ten_tweets:\n",
    "        print(\n",
    "            f\"URL: {data['url']}, Tweets: {data['tweet_count']}, Title: {data['title']}, Abstract: {data['abstract']}, Subject Area: {data['subject_area']}\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use api for finding the best https://api.biorxiv.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first find top ten papers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw response text: <br />\n",
      "<b>Warning</b>:  Undefined array key \"d2\" in <b>/var/www/vhosts/api.biorxiv.org/inc_published_details.php</b> on line <b>9</b><br />\n",
      "{\"messages\":[{\"status\":\"no articles found for 0 \"}], \"collection\":[]}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Error decoding JSON: Expecting value: line 1 column 1 (char 0)\n",
      "top_ten_papers []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from datetime import datetime\n",
    "\n",
    "# Function to get top ten most recent papers\n",
    "def get_top_ten_recent_papers():\n",
    "    url = \"https://api.biorxiv.org/pubs/biorxiv/0\"  # Fetch the most recent papers\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Print the raw response text for debugging\n",
    "    print(\"Raw response text:\", response.text)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        try:\n",
    "            data = response.json()\n",
    "            papers = data.get('collection', [])\n",
    "            \n",
    "            # Sort papers by views or downloads (assuming 'views' or 'downloads' key exists)\n",
    "            top_ten_papers = sorted(papers, key=lambda x: x.get('views', 0), reverse=True)[:10]\n",
    "            \n",
    "            return top_ten_papers\n",
    "        except ValueError as e:\n",
    "            print(\"Error decoding JSON:\", e)\n",
    "            return []\n",
    "    else:\n",
    "        print(\"Error fetching data:\", response.status_code)\n",
    "        return []\n",
    "\n",
    "# Example usage for recent papers\n",
    "top_ten_papers = get_top_ten_recent_papers()\n",
    "print(\"top_ten_papers\", top_ten_papers)\n",
    "for paper in top_ten_papers:\n",
    "    print(f\"Title: {paper['preprint_title']}, Views: {paper.get('views', 0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to rewrite the logic for bioarxiv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
